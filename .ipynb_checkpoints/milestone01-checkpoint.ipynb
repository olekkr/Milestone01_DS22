{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe88cff8",
   "metadata": {},
   "source": [
    "## ass:\n",
    "X Read the CSV file  \n",
    "Manually inspect the data to get an idea of potential problems that need to be fixed.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06e58dc",
   "metadata": {},
   "source": [
    "## Imports: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4da23f5d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5d1f80",
   "metadata": {},
   "source": [
    "## Downloading csv from internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41d8c34f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csvdata = requests.get(\"https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv\")\n",
    "csvdata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da90caf",
   "metadata": {},
   "source": [
    "## Saving csv as a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8dd75aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write csvfile \n",
    "with open(\"rawdata.csv\",\"wb\") as file:\n",
    "    file.write(csvdata.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf58e279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Unnamed: 0     id                domain        type  \\\n",
      "0             0    141               awm.com  unreliable   \n",
      "1             1    256     beforeitsnews.com        fake   \n",
      "2             2    700           cnnnext.com  unreliable   \n",
      "3             3    768               awm.com  unreliable   \n",
      "4             4    791  bipartisanreport.com   clickbait   \n",
      "..          ...    ...                   ...         ...   \n",
      "245         245  39259     beforeitsnews.com        fake   \n",
      "246         246  39468     beforeitsnews.com        fake   \n",
      "247         247  39477       www.newsmax.com         NaN   \n",
      "248         248  39550       www.newsmax.com         NaN   \n",
      "249         249  39558       www.newsmax.com         NaN   \n",
      "\n",
      "                                                   url  \\\n",
      "0    http://awm.com/church-congregation-brings-gift...   \n",
      "1    http://beforeitsnews.com/awakening-start-here/...   \n",
      "2    http://www.cnnnext.com/video/18526/never-hike-...   \n",
      "3    http://awm.com/elusive-alien-of-the-sea-caught...   \n",
      "4    http://bipartisanreport.com/2018/01/21/trumps-...   \n",
      "..                                                 ...   \n",
      "245  http://beforeitsnews.com/economy/2017/12/priso...   \n",
      "246  http://beforeitsnews.com/diy/2017/11/4-useful-...   \n",
      "247  https://www.newsmax.com/politics/michael-hayde...   \n",
      "248  https://www.newsmax.com/newsfront/antonio-saba...   \n",
      "249  https://www.newsmax.com/newsfront/bill-clinton...   \n",
      "\n",
      "                                               content  \\\n",
      "0    Sometimes the power of Christmas will make you...   \n",
      "1    AWAKENING OF 12 STRANDS of DNA – “Reconnecting...   \n",
      "2    Never Hike Alone: A Friday the 13th Fan Film U...   \n",
      "3    When a rare shark was caught, scientists were ...   \n",
      "4    Donald Trump has the unnerving ability to abil...   \n",
      "..                                                 ...   \n",
      "245  Prison for Rahm, God’s Work And Many Others\\n\\...   \n",
      "246  4 Useful Items for Your Tiny Home\\n\\nHeadline:...   \n",
      "247  Former CIA Director Michael Hayden said Thursd...   \n",
      "248  Antonio Sabato Jr. says Hollywood's liberal el...   \n",
      "249  Former U.S. President Bill Clinton on Monday c...   \n",
      "\n",
      "                     scraped_at                 inserted_at  \\\n",
      "0    2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
      "1    2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
      "2    2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
      "3    2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
      "4    2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
      "..                          ...                         ...   \n",
      "245  2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n",
      "246  2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n",
      "247  2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n",
      "248  2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n",
      "249  2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n",
      "\n",
      "                     updated_at  \\\n",
      "0    2018-02-02 01:19:41.756664   \n",
      "1    2018-02-02 01:19:41.756664   \n",
      "2    2018-02-02 01:19:41.756664   \n",
      "3    2018-02-02 01:19:41.756664   \n",
      "4    2018-02-02 01:19:41.756664   \n",
      "..                          ...   \n",
      "245  2018-02-02 01:19:41.756664   \n",
      "246  2018-02-02 01:19:41.756664   \n",
      "247  2018-02-02 01:19:41.756664   \n",
      "248  2018-02-02 01:19:41.756664   \n",
      "249  2018-02-02 01:19:41.756664   \n",
      "\n",
      "                                                 title          authors  \\\n",
      "0    Church Congregation Brings Gift to Waitresses ...      Ruth Harris   \n",
      "1    AWAKENING OF 12 STRANDS of DNA – “Reconnecting...     Zurich Times   \n",
      "2    Never Hike Alone - A Friday the 13th Fan Film ...              NaN   \n",
      "3    Elusive ‘Alien Of The Sea ‘ Caught By Scientis...  Alexander Smith   \n",
      "4    Trump’s Genius Poll Is Complete & The Results ...  Gloria Christie   \n",
      "..                                                 ...              ...   \n",
      "245        Prison for Rahm, God’s Work And Many Others              NaN   \n",
      "246                  4 Useful Items for Your Tiny Home        Dimitry K   \n",
      "247  Michael Hayden: We Should Be 'Frightened' by T...      Todd Beamon   \n",
      "248  Antonio Sabato Jr.: It's Oprah or Bust for Hol...    Bill Hoffmann   \n",
      "249  Bill Clinton Calls for Release of Reuters Jour...              NaN   \n",
      "\n",
      "     keywords                                      meta_keywords  \\\n",
      "0         NaN                                               ['']   \n",
      "1         NaN                                               ['']   \n",
      "2         NaN                                               ['']   \n",
      "3         NaN                                               ['']   \n",
      "4         NaN                                               ['']   \n",
      "..        ...                                                ...   \n",
      "245       NaN                                               ['']   \n",
      "246       NaN                                               ['']   \n",
      "247       NaN  ['michael hayden', 'sthole countries', 'daca',...   \n",
      "248       NaN  ['antonio sabato jr', 'oprah winfrey', 'presid...   \n",
      "249       NaN  ['bill clinton', 'myanmar', 'calls', 'release'...   \n",
      "\n",
      "                                      meta_description  \\\n",
      "0                                                  NaN   \n",
      "1                                                  NaN   \n",
      "2    Never Hike Alone: A Friday the 13th Fan Film  ...   \n",
      "3                                                  NaN   \n",
      "4                                                  NaN   \n",
      "..                                                 ...   \n",
      "245                                                NaN   \n",
      "246                                                NaN   \n",
      "247  President Donald Trump's reported remarks abou...   \n",
      "248  Antonio Sabato Jr. says Hollywood's liberal el...   \n",
      "249  Former U.S. President Bill Clinton Calls for R...   \n",
      "\n",
      "                                                  tags  summary  \n",
      "0                                                  NaN      NaN  \n",
      "1                                                  NaN      NaN  \n",
      "2                                                  NaN      NaN  \n",
      "3                                                  NaN      NaN  \n",
      "4                                                  NaN      NaN  \n",
      "..                                                 ...      ...  \n",
      "245                                                NaN      NaN  \n",
      "246                                                NaN      NaN  \n",
      "247  Homeland Security, Trump Administration, Immig...      NaN  \n",
      "248  Trump Administration, ISIS/Islamic State, News...      NaN  \n",
      "249  Donald Trump, Russia, Trump Administration, Gu...      NaN  \n",
      "\n",
      "[250 rows x 16 columns]\n"
     ]
    }
   ],
   "source": [
    "# Parse csv into dataframe\n",
    "df = pd.read_csv ('rawdata.csv')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0288fd77",
   "metadata": {},
   "source": [
    "### First takeaways:\n",
    "* Data is in csv format, means we have to parse it.\n",
    "* Newlines are used for each row, commas are used to seperate each field.\n",
    "* Some commas exist in the text, though they should be within sets of quotiationmarks.\n",
    "* A lot of empty fields\n",
    "* A lot of duplicate whitespace and newlines\n",
    "* browsing the data has shown that summary is always None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ba3712b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0            int64\n",
       "id                    int64\n",
       "domain               object\n",
       "type                 object\n",
       "url                  object\n",
       "content              object\n",
       "scraped_at           object\n",
       "inserted_at          object\n",
       "updated_at           object\n",
       "title                object\n",
       "authors              object\n",
       "keywords            float64\n",
       "meta_keywords        object\n",
       "meta_description     object\n",
       "tags                 object\n",
       "summary             float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary is float, since its unspecefied. \n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849e8772",
   "metadata": {},
   "source": [
    "Clean the data. First, we'll try to do this manually, by writing our own clean_text() function that uses regular expressions. The function should take raw text as input and return a version of the text with the following modifications:  \n",
    "        all words must be lowercased\n",
    "        it should not contain multiple white spaces, tabs or new lines\n",
    "        numbers, dates, emails and urls should be replaced by \"<NUM>\", \"<DATE>\", \"<EMAIL>\" AND \"<URL>\", respectively. Note that replacing dates with <DATE> is particularly tricky as dates can be expressed in many forms. It's ok to to just choose one or a few common date formats present in the data set and only replace those. (Be careful about tokenizing <> symbols because these are punctuation in most Tokenizers).\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f817b068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# each aspect of clean_text can be split up into seperate functions.\n",
    "\n",
    "# function that takes string and returns lowercased string\n",
    "def str2Lower(inputStr:str):\n",
    "    # \"repl\" substitution function for re.sub()\n",
    "    def upper2Lower(match:re.Match):\n",
    "        Ls = string.ascii_lowercase\n",
    "        Us = string.ascii_uppercase\n",
    "        return Ls[Us.index(match.group(0))]\n",
    "    # returns string after substituion operation\n",
    "    return re.sub(r\"([A-Z])\", upper2Lower, inputStr)\n",
    "\n",
    "# function that takes string with consecutive spaces and returns stripped string.\n",
    "def stripStr(inputStr:str):\n",
    "    def repl(match:re.Match):\n",
    "        return \" \"\n",
    "    return re.sub(r\"(\\s{2,})\", repl, inputStr)\n",
    "\n",
    "\n",
    "# function wich inserts <tags>\n",
    "def insertSymbols(inputStr:str):\n",
    "    text = inputStr\n",
    "    \n",
    "    # insert <DATE>\n",
    "    # DD-MM-YY format\n",
    "    text = re.sub(r\"[0-3]\\d[/-][01]\\d[/-]\\d\\d\", (lambda _ : \"<DATE>\"), text)\n",
    "    \n",
    "    # insert <NUM>\n",
    "    text = re.sub(r\"([\\d]+[\\d,.]*)\", (lambda _ : \"<NUM>\"), text)\n",
    "    \n",
    "    # insert <EMAIL>\n",
    "    # RFC 5322 compliant regex\n",
    "    emailPat = \"\"\"(?:[a-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\\.[a-z0-9!#$%&'*+/=?^_`{|}~-]+)*|\"(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])*\")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\\[(?:(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9]))\\.){3}(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9])|[a-z0-9-]*[a-z0-9]:(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\])\"\"\"\n",
    "    text = re.sub(emailPat, (lambda _ : \"<EMAIL>\"), text)\n",
    "    \n",
    "    # insert <URL>\n",
    "    # URL Pattern, requres http/https \n",
    "    URLPat = \"\"\"https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)\"\"\"\n",
    "    text = re.sub(URLPat, (lambda _ : \"<URL>\"), text)\n",
    "    return text\n",
    "\n",
    "def clean_text(inputStr:str):\n",
    "    text = inputStr\n",
    "    text = str2Lower(text)\n",
    "    text = stripStr(text)\n",
    "    text = insertSymbols(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d979e108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: abasda;l3;'123l1'\n",
      "test: absd   absd :;';01932,./' howdy@hello.dk date: 14/12/01 02-03-22, https://regexr.com/\n",
      "test: Ab asd a;l3 ;'1 23l1'\n",
      "test: absd AbsD :;';01932,./' howdy@hello.dk date: 14/12/01 02-03-22, https://regexr.com/\n",
      "test: absd   AbsD :;';<NUM>/' <EMAIL> date: <DATE> <DATE>, <URL>\n",
      "test: absd absd :;';<NUM>/' <EMAIL> date: <DATE> <DATE>, <URL>\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "testStr = r\"absd   AbsD :;';01932,./' howdy@hello.dk date: 14/12/01 02-03-22, https://regexr.com/\"\n",
    "print(\"test:\", str2Lower(\"Abasda;l3;'123l1'\"))\n",
    "print(\"test:\", str2Lower(testStr))\n",
    "\n",
    "\n",
    "print(\"test:\", stripStr(\"Ab  asd  a;l3   ;'1  23l1'\"))\n",
    "print(\"test:\", stripStr(testStr))\n",
    "\n",
    "print(\"test:\", insertSymbols(testStr))\n",
    "\n",
    "print(\"test:\", clean_text(testStr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973bde18",
   "metadata": {},
   "source": [
    "### Better clean_text(str)\n",
    "Now, let's try to use a library for cleaning the data. The clean-text module (https://pypi.org/project/clean-text/ (Links to an external site.)) provides out-of-the-box functionality for much of the cleaning we did in the previous exercise (pip install clean-text). Use it to implement the same cleaning steps as in your own clean_text implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f65c17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/olekkr/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# new imports\n",
    "from cleantext import clean\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01ce157d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sometimes the power of Christmas will ma'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text_new(text:str):\n",
    "    return clean(text,\n",
    "        fix_unicode=True,               # fix various unicode errors\n",
    "        to_ascii=True,                  # transliterate to closest ASCII representation\n",
    "        lower=True,                     # lowercase text\n",
    "        no_line_breaks=False,           # fully strip line breaks as opposed to only normalizing them\n",
    "        no_urls=True,                  # replace all URLs with a special token\n",
    "        no_emails=True,                # replace all email addresses with a special token\n",
    "        no_phone_numbers=False,         # replace all phone numbers with a special token\n",
    "        no_numbers=True,               # replace all numbers with a special token\n",
    "        no_digits=False,                # replace all digits with a special token\n",
    "        no_currency_symbols=False,      # replace all currency symbols with a special token\n",
    "        no_punct=False,                 # remove punctuations\n",
    "        replace_with_punct=\"\",          # instead of removing punctuations you may replace them\n",
    "        replace_with_url=\"<URL>\",\n",
    "        replace_with_email=\"<EMAIL>\",\n",
    "        replace_with_phone_number=\"<PHONE>\",\n",
    "        replace_with_number=\"<NUM>\",\n",
    "        replace_with_digit=\"0\",\n",
    "        replace_with_currency_symbol=\"<CUR>\",\n",
    "        lang=\"en\"                       # set to 'de' for German special handling\n",
    "    )\n",
    "(df.content[0])[0:40] # example ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e10503",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aef5f675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'awakening of <num> strands of dna - \"reconnecting with you\" movie\\n% of readers think this story is fact. add your two cents.\\nheadline: bitcoin & blockchain searches exceed trump! blockchain stocks are next!\\n[january <num>, <num> - zurichtimes.net]\\nas miles johnston was giving update, it was another case of strange synchronicities of goodness hidden inside of tests and trials, like a follow the whiterabbit down the rabbit hole type of exercise.\\nin researching the <num> strands of dna we came across some articles, one in particular was as a strange synchronicity written exactly <num> year ago on the same topic.\\n<url>\\n<url>\\nwhat are the <num> strands of our dna and why is a war against our dna?\\ntrailer for awakening of <num> strands\\nthe full video is only available as a paid video on vimeo.\\nawakening of <num> strands - \"reconnecting with you\"\\nvimeo.com/ondemand/awakeningof12strands\\nawakening of <num> strands - \"reconnecting with you\". from sandra daroy on vimeo.\\nwe have not watched the full video, but based on the strange synchronicities present within we suggest you use your discernment as always necessary in these end times.\\n<url>'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply function on our dataset\n",
    "df.content = df.content.apply(clean_text_new)\n",
    "df.content[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e2c917",
   "metadata": {},
   "source": [
    "Now that we are done cleaning, we can start process the text. The nltk library (https://www.nltk.org/ (Links to an external site.)) has built-in support for many of the most common operations. Try to:\n",
    "Tokenize the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae441841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sometimes the power of christmas will make you do wild and wonderful things.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testSentence = df.content[0][0:76]\n",
    "testSentence "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71276201",
   "metadata": {},
   "source": [
    "## Stopword removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "faca680c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genStopwordStats (data:pd.Series):\n",
    "    def countTokens (tokens:list[str]):\n",
    "        if tokens is None:\n",
    "            return 0\n",
    "        return len(tokens)\n",
    "    \n",
    "    def tokenize (text:str):\n",
    "        return list(set(word_tokenize(text)))\n",
    "    \n",
    "    def removeStopWords(tokens:list[str]):\n",
    "        sWords = stopwords.words('english')\n",
    "        return list(set([token for token in tokens if not token in sWords]))\n",
    "    \n",
    "    tokensWithStops = pd.Series(data.apply(tokenize), name = \"tokensWithStops\")\n",
    "    countWithStops = pd.Series(tokensWithStops.apply(countTokens), name = \"countWithStops\")\n",
    "    tokensNoStops = pd.Series(tokensWithStops.apply(removeStopWords), name = \"tokensNoStops\")\n",
    "    countNoStops = pd.Series(tokensNoStops.apply(countTokens), name = \"countNoStops\")\n",
    "    reductionRate = pd.Series((countWithStops-countNoStops)/countNoStops, name=\"reductionRate\")\n",
    "    \n",
    "    newData = pd.concat([\n",
    "        tokensWithStops, countWithStops, \n",
    "        tokensNoStops, countNoStops,reductionRate], axis=1)\n",
    "\n",
    "    return newData\n",
    "stopWordStats = genStopwordStats(df.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b02b1d",
   "metadata": {},
   "source": [
    "Remove stopwords and compute the size of the vocabulary. \n",
    "Compute the reduction rate of the vocabulary size after removing stopwords.\n",
    "Remove word variations with stemming and compute the size of the vocabulary. Compute the reduction rate of the vocabulary size after stemming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3db882",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4af15d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ps = PorterStemmer()\n",
    "def stemStats (data:pd.Series):\n",
    "    \n",
    "    def countTokens (tokens:list[str]):\n",
    "        if tokens is None:\n",
    "            return 0\n",
    "        return len(tokens)\n",
    "    \n",
    "    # function to be applied on each element in series\n",
    "    def stem (tokens:list[str]):\n",
    "        return list(set([ps.stem(token) for token in tokens]))\n",
    " \n",
    "    \n",
    "    tokenLenUnstemmed = pd.Series(data.apply(countTokens), name=\"tokenLenUnstemmed\")\n",
    "    stemmedTokens = pd.Series(data.apply(stem), name=\"stemmedTokens\")\n",
    "    tokenLenStemmed = pd.Series(stemmedTokens.apply(countTokens), name=\"tokenLenStemmed\")\n",
    "    stemmingReductionR = pd.Series((tokenLenUnstemmed-tokenLenStemmed)/(tokenLenStemmed), name=\"reductionRate\")\n",
    "    \n",
    "    return pd.concat([data,stemmedTokens,tokenLenUnstemmed,tokenLenStemmed,stemmingReductionR],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6de504a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "169"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "stemmedStats = stemStats(stopWordStats.tokensNoStops)\n",
    "len(stopWordStats.tokensNoStops[0])\n",
    "#len(\n",
    "#[ps.stem(token) for token in stopWordStats.tokensNoStops[0]]\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "718eadc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokensNoStops</th>\n",
       "      <th>stemmedTokens</th>\n",
       "      <th>tokenLenUnstemmed</th>\n",
       "      <th>tokenLenStemmed</th>\n",
       "      <th>reductionRate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[still, preacher, fashion, -, lived, room, shi...</td>\n",
       "      <td>[gener, still, preacher, fashion, famili, -, r...</td>\n",
       "      <td>169</td>\n",
       "      <td>162</td>\n",
       "      <td>0.043210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-, exercise, articles, within, next, dna, vim...</td>\n",
       "      <td>[-, avail, reader, within, next, dna, research...</td>\n",
       "      <td>91</td>\n",
       "      <td>90</td>\n",
       "      <td>0.011111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[crew, covington, studios, wardrobe, j.d, disa...</td>\n",
       "      <td>[still, crew, disclaim, co-produc, enchilada, ...</td>\n",
       "      <td>232</td>\n",
       "      <td>224</td>\n",
       "      <td>0.035714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[fashion, fisheries, -, vessel, extendable, kn...</td>\n",
       "      <td>[uniqu, fashion, -, blunder, vessel, known, da...</td>\n",
       "      <td>196</td>\n",
       "      <td>184</td>\n",
       "      <td>0.065217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[spanish, angerer, saw, greatest, random, arou...</td>\n",
       "      <td>[imag, spanish, abl, ask, million, saw, greate...</td>\n",
       "      <td>122</td>\n",
       "      <td>118</td>\n",
       "      <td>0.033898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>[....., flagged, pussy, room, ratings, insuran...</td>\n",
       "      <td>[....., room, eunuch, civil, allegedli, outlin...</td>\n",
       "      <td>494</td>\n",
       "      <td>454</td>\n",
       "      <td>0.088106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>[rely, stretch, drying, degree, set, worth, we...</td>\n",
       "      <td>[basi, stretch, set, room, extra, dri, worth, ...</td>\n",
       "      <td>223</td>\n",
       "      <td>207</td>\n",
       "      <td>0.077295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>[example, newspaper, idea, judgment, immigrati...</td>\n",
       "      <td>[immigr, restor, ask, idea, angel, judgment, u...</td>\n",
       "      <td>100</td>\n",
       "      <td>98</td>\n",
       "      <td>0.020408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>[nobody, 'me, 26th, room, -, ask, plans, crowd...</td>\n",
       "      <td>[gener, 'me, 26th, room, -, ask, avail, crowd,...</td>\n",
       "      <td>197</td>\n",
       "      <td>188</td>\n",
       "      <td>0.047872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>[,, detention, -, clinton, rule, second, unacc...</td>\n",
       "      <td>[,, reuter, little-us, report, call, -, clinto...</td>\n",
       "      <td>65</td>\n",
       "      <td>62</td>\n",
       "      <td>0.048387</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         tokensNoStops  \\\n",
       "0    [still, preacher, fashion, -, lived, room, shi...   \n",
       "1    [-, exercise, articles, within, next, dna, vim...   \n",
       "2    [crew, covington, studios, wardrobe, j.d, disa...   \n",
       "3    [fashion, fisheries, -, vessel, extendable, kn...   \n",
       "4    [spanish, angerer, saw, greatest, random, arou...   \n",
       "..                                                 ...   \n",
       "245  [....., flagged, pussy, room, ratings, insuran...   \n",
       "246  [rely, stretch, drying, degree, set, worth, we...   \n",
       "247  [example, newspaper, idea, judgment, immigrati...   \n",
       "248  [nobody, 'me, 26th, room, -, ask, plans, crowd...   \n",
       "249  [,, detention, -, clinton, rule, second, unacc...   \n",
       "\n",
       "                                         stemmedTokens  tokenLenUnstemmed  \\\n",
       "0    [gener, still, preacher, fashion, famili, -, r...                169   \n",
       "1    [-, avail, reader, within, next, dna, research...                 91   \n",
       "2    [still, crew, disclaim, co-produc, enchilada, ...                232   \n",
       "3    [uniqu, fashion, -, blunder, vessel, known, da...                196   \n",
       "4    [imag, spanish, abl, ask, million, saw, greate...                122   \n",
       "..                                                 ...                ...   \n",
       "245  [....., room, eunuch, civil, allegedli, outlin...                494   \n",
       "246  [basi, stretch, set, room, extra, dri, worth, ...                223   \n",
       "247  [immigr, restor, ask, idea, angel, judgment, u...                100   \n",
       "248  [gener, 'me, 26th, room, -, ask, avail, crowd,...                197   \n",
       "249  [,, reuter, little-us, report, call, -, clinto...                 65   \n",
       "\n",
       "     tokenLenStemmed  reductionRate  \n",
       "0                162       0.043210  \n",
       "1                 90       0.011111  \n",
       "2                224       0.035714  \n",
       "3                184       0.065217  \n",
       "4                118       0.033898  \n",
       "..               ...            ...  \n",
       "245              454       0.088106  \n",
       "246              207       0.077295  \n",
       "247               98       0.020408  \n",
       "248              188       0.047872  \n",
       "249               62       0.048387  \n",
       "\n",
       "[250 rows x 5 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmedStats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2605360",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final = pd.concat([stopWordStats,stemmedStats], axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
