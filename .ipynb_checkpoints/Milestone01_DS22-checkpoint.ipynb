{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8bbb817",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "\n",
    "%\n",
    "If you haven't already done so, the first task is to form a group of 2-4 people. There is an announcement on Absalon describing how to do this. Make sure that you list the names of all members of the group at the top of the jupyter notebook along with your group number.\n",
    "%\n",
    "\n",
    "## Group members:\n",
    "* Oleksandr kryshtalov [mdq842]  \n",
    "* Ulrik Bjørn Meelby [na]  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33adf0d",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "%\n",
    "For our fake news predictor, we will be using the FakeNewsCorpus dataset as our primary dataset. \n",
    "It is available from this github repository:, where you can also find information about how the data is collected, the available fields, etc. In this first milestone, we will work only on a small subset of the FakeNewsCorpus dataset. \n",
    "Your first task is to retrieve this subset from https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv and structure/process/clean it. Describe which procedures (and which libraries) you used and why they are appropriate.\n",
    "%\n",
    "\n",
    "In order to clean the data we need to first know what the primary goal of cleaning it is. \n",
    "In task 3 we have chosen to analyze following questions:\n",
    "* Are there words that have a higher frequency in fake-news articles over trustworthy ones, and what are those?\n",
    "* What are the meta_keywords that have the highest inclination to be fake?\n",
    "* Observe zipf law on english text with given dataset.\n",
    "\n",
    "All observations require text tokenization, and a way to clean and stem them, in order to not differentiate between words, eg. word and words.\n",
    "\n",
    "For the cleaning of the text we will be using cleatext.\n",
    "nltk will be used for tokinization via word_tokenize, and for stemming via the porter stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41c7666f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports:\n",
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "from cleantext import clean\n",
    "import nltk\n",
    "#from nltk.corpus import stopwords\n",
    "#from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e007ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We get the data from the web \n",
    "csvdata = requests.get(\"https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv\")\n",
    "csvdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ad09325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>domain</th>\n",
       "      <th>type</th>\n",
       "      <th>content</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>meta_keywords</th>\n",
       "      <th>meta_description</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>141</td>\n",
       "      <td>awm.com</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>Sometimes the power of Christmas will make you...</td>\n",
       "      <td>Church Congregation Brings Gift to Waitresses ...</td>\n",
       "      <td>Ruth Harris</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>256</td>\n",
       "      <td>beforeitsnews.com</td>\n",
       "      <td>fake</td>\n",
       "      <td>AWAKENING OF 12 STRANDS of DNA – “Reconnecting...</td>\n",
       "      <td>AWAKENING OF 12 STRANDS of DNA – “Reconnecting...</td>\n",
       "      <td>Zurich Times</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>700</td>\n",
       "      <td>cnnnext.com</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>Never Hike Alone: A Friday the 13th Fan Film U...</td>\n",
       "      <td>Never Hike Alone - A Friday the 13th Fan Film ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>Never Hike Alone: A Friday the 13th Fan Film  ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id             domain        type  \\\n",
       "0  141            awm.com  unreliable   \n",
       "1  256  beforeitsnews.com        fake   \n",
       "2  700        cnnnext.com  unreliable   \n",
       "\n",
       "                                             content  \\\n",
       "0  Sometimes the power of Christmas will make you...   \n",
       "1  AWAKENING OF 12 STRANDS of DNA – “Reconnecting...   \n",
       "2  Never Hike Alone: A Friday the 13th Fan Film U...   \n",
       "\n",
       "                                               title       authors  \\\n",
       "0  Church Congregation Brings Gift to Waitresses ...   Ruth Harris   \n",
       "1  AWAKENING OF 12 STRANDS of DNA – “Reconnecting...  Zurich Times   \n",
       "2  Never Hike Alone - A Friday the 13th Fan Film ...           NaN   \n",
       "\n",
       "  meta_keywords                                   meta_description tags  \n",
       "0          ['']                                                NaN  NaN  \n",
       "1          ['']                                                NaN  NaN  \n",
       "2          ['']  Never Hike Alone: A Friday the 13th Fan Film  ...  NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write csvfile\n",
    "with open(\"rawdata.csv\",\"wb\") as file:\n",
    "    file.write(csvdata.content)\n",
    "\n",
    "# Parse csv into dataframe\n",
    "dfRaw = pd.read_csv ('rawdata.csv')\n",
    "\n",
    "# drop some tables for brevity:\n",
    "df = dfRaw.iloc[: , 1:]\n",
    "df = df.drop(columns =[ \"url\", \"scraped_at\", \"scraped_at\", \"inserted_at\", \"updated_at\", \"keywords\", \"summary\"])\n",
    "\n",
    "# have a peek at the data:\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3aaee54b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fake          155\n",
       "conspiracy     31\n",
       "political      23\n",
       "unreliable      6\n",
       "bias            6\n",
       "junksci         6\n",
       "unknown         6\n",
       "reliable        3\n",
       "clickbait       1\n",
       "hate            1\n",
       "Name: type, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d45a8c7",
   "metadata": {},
   "source": [
    "## Cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e12ca925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                    NaN\n",
       "1                                                    NaN\n",
       "2      Never Hike Alone: A Friday the 13th Fan Film  ...\n",
       "3                                                    NaN\n",
       "4                                                    NaN\n",
       "                             ...                        \n",
       "245                                                  NaN\n",
       "246                                                  NaN\n",
       "247    President Donald Trump's reported remarks abou...\n",
       "248    Antonio Sabato Jr. says Hollywood's liberal el...\n",
       "249    Former U.S. President Bill Clinton Calls for R...\n",
       "Name: meta_description, Length: 250, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We want to make a pd.Series -> pd.Series function that cleans all the text\n",
    "# lowercases everything removes urls, emails etc.\n",
    "\n",
    "def clean_text(textSeries:pd.Series) -> pd.Series:\n",
    "    def clean_func (text):\n",
    "        return clean(text,\n",
    "            fix_unicode=True,            \n",
    "            to_ascii=True,               \n",
    "            lower=True,                  \n",
    "            no_line_breaks=True,        \n",
    "            no_urls=True,                \n",
    "            no_emails=True,              \n",
    "            no_phone_numbers=False,      \n",
    "            no_numbers=True,             \n",
    "            no_punct=True,              \n",
    "            replace_with_punct=\"\",       \n",
    "            replace_with_url=\"oURL\", # clean_text lowers after tagging for some reason???\n",
    "            replace_with_email=\"oEMAIL\",\n",
    "            replace_with_phone_number=\"oPHONE\",\n",
    "            replace_with_number=\"oNUM\",\n",
    "            replace_with_digit=\"0\",\n",
    "            lang=\"en\")\n",
    "    return textSeries.apply(clean_func)\n",
    "            \n",
    "df.content = clean_text(df.content)\n",
    "df[\"meta_description\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad34b170",
   "metadata": {},
   "source": [
    "## Tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36def21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/olekkr/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt') # fix\n",
    "\n",
    "# Same as before a pd.Series -> pd.Series function.\n",
    "def tokenize(textSeries:pd.Series) -> pd.Series:\n",
    "    return pd.Series(textSeries.apply(nltk.tokenize.wordpunct_tokenize),name=\"tokens\")\n",
    "df[\"tokens\"] = tokenize(df.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6e2494",
   "metadata": {},
   "source": [
    "## Removal of stopwords:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11efdce9",
   "metadata": {},
   "source": [
    "## Stemming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0ecb79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stems all the tokens\n",
    "def stem (data:pd.Series)-> pd.Series:\n",
    "    ps = PorterStemmer()\n",
    "    # function to be applied on each element in series\n",
    "    def stem (tokens):\n",
    "        return [ps.stem(token) for token in tokens]\n",
    "    return pd.Series(data.apply(stem))\n",
    "\n",
    "# removal of duplicates\n",
    "def dupeRemove (tokens:pd.Series) -> pd.Series:\n",
    "    def unDupe (tokens):\n",
    "        return list(set(tokens))\n",
    "    return tokens.apply(unDupe)\n",
    "\n",
    "\n",
    "\n",
    "df[\"stemmed_with_dupes\"] = stem(df.tokens)\n",
    "df[\"stemmed\"] = dupeRemove(df.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b88ecb0",
   "metadata": {},
   "source": [
    "# Task 3\n",
    "%\\\n",
    "Now try to explore the FakeNewsCorpus dataset. Make at least three non-trivial observations/discoveries about the data. These observations could be related to outliers, artefacts, or even better: genuinely interesting patterns in the data that could potentially be used for fake-news detection. Examples of simple observations could be how many missing values there are in particular columns - or what the distribution over domains is. Be creative! :).  \n",
    "%\n",
    "\n",
    "We found some observations we wanted to look into in Task 2: \n",
    "1. Are there words that have a higher frequency in fake-news articles over trustworthy ones, and what are those?\n",
    "2. What are the meta_keywords that have the highest frequency in biasd/unreliable news?\n",
    "3. Observe zipf law on english text with given dataset.\n",
    "\n",
    "These tasks can then be split into smaller tasks:\n",
    "### 1\n",
    "* Generate BOW for fake and credibile articles seperatly. \n",
    "* For each word find credible/fake delta of the frequencies. \n",
    "\n",
    "\n",
    "### 2\n",
    "* Create BOW for meta keywords for fakes\n",
    "* Sort \n",
    "* Visualize this coefficiant.\n",
    "\n",
    "\n",
    "## 3\n",
    "* Create a vocabulary for the entire dataset \n",
    "* calculate bag of words for all text. \n",
    "* Sort BOW by frequency. \n",
    "* plot. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a430b7",
   "metadata": {},
   "source": [
    "## Creating BOWs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "323bf11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 out of 10 \n",
      "Done\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>the</td>\n",
       "      <td>299</td>\n",
       "      <td>0.096141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>onum</td>\n",
       "      <td>175</td>\n",
       "      <td>0.056270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>of</td>\n",
       "      <td>155</td>\n",
       "      <td>0.049839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>to</td>\n",
       "      <td>112</td>\n",
       "      <td>0.036013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>in</td>\n",
       "      <td>105</td>\n",
       "      <td>0.033762</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     word  count  frequency\n",
       "19    the    299   0.096141\n",
       "94   onum    175   0.056270\n",
       "39     of    155   0.049839\n",
       "1      to    112   0.036013\n",
       "109    in    105   0.033762"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In the following section we have BOW code, to generate bags of words\n",
    "# and combining them. As previous we well be using pd.Series as out datatype.\n",
    "# The code takes a Long time to complete, might want to skip this section and \n",
    "# instead unpickle. \n",
    "\n",
    "\n",
    "# generate BOW given a set of tokens(with dupes).\n",
    "def BOWGen(tokenss:pd.Series) -> pd.DataFrame: # with 2 cols for word and count\n",
    "    \n",
    "    #generate Bow for 1 list of tokens\n",
    "    def _BOWGen(tokens:list) -> (list,list): # with 2 cols for word and count\n",
    "        vocab = []\n",
    "        freqs = []\n",
    "        for token in tokens:\n",
    "            if token not in vocab:\n",
    "                vocab.append(token)\n",
    "                freqs.append(1)\n",
    "            else:\n",
    "                # increment frequncy counter by one.\n",
    "                freqs[vocab.index(token)] += 1\n",
    "        return vocab,freqs\n",
    "    return tokenss.apply(_BOWGen)\n",
    "\n",
    "\n",
    "def BOWGenGlobal(BOWs:pd.Series) -> pd.DataFrame:\n",
    "    def mergeBOWs(a:(list,list), b):\n",
    "        vocab, freqs = a\n",
    "        for (word, count) in zip(b[0],b[1]):\n",
    "            if word not in vocab:\n",
    "                vocab.append(word)\n",
    "                freqs.append(count)\n",
    "            else:\n",
    "                freqs[vocab.index(word)] += count\n",
    "        return vocab, freqs\n",
    "    \n",
    "    BOWList = [i for (_, i) in BOWs.items()]\n",
    "    \n",
    "    # accumulator\n",
    "    _BOW = ([],[])\n",
    "    for idx, BOW in enumerate(BOWList):\n",
    "        _BOW = mergeBOWs(BOW, _BOW)\n",
    "        if (idx) % 20 == 0:\n",
    "            print(f\"{idx} out of {len(BOWList)} \")\n",
    "    out = pd.DataFrame((_BOW)).transpose()\n",
    "    out.columns = [\"word\", \"count\"]\n",
    "    out[\"count\"] = out[\"count\"].astype(int)\n",
    "    out[\"frequency\"] = out[\"count\"]/out.size\n",
    "    print(\"Done\")\n",
    "    return out\n",
    "      \n",
    "# test \n",
    "BOWs = BOWGenGlobal(BOWGen(df.tokens.iloc[0:10]))\n",
    "BOWs.sort_values(\"count\",ascending=False).iloc[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7d8025",
   "metadata": {},
   "source": [
    "## Generating all the BOWs we need\n",
    "##### This takes a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45ca2b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 out of 250 \n",
      "20 out of 250 \n",
      "40 out of 250 \n",
      "60 out of 250 \n",
      "80 out of 250 \n",
      "100 out of 250 \n",
      "120 out of 250 \n",
      "140 out of 250 \n",
      "160 out of 250 \n",
      "180 out of 250 \n",
      "200 out of 250 \n",
      "220 out of 250 \n",
      "240 out of 250 \n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "use_pickles = False #flag for loading/saving pickles\n",
    "\n",
    "if not use_pickles:\n",
    "    # BOW Whole text\n",
    "    BOW_Whole_text = BOWGenGlobal(BOWGen(df.tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02bb0cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 out of 247 \n",
      "20 out of 247 \n",
      "40 out of 247 \n",
      "60 out of 247 \n",
      "80 out of 247 \n",
      "100 out of 247 \n",
      "120 out of 247 \n",
      "140 out of 247 \n",
      "160 out of 247 \n",
      "180 out of 247 \n",
      "200 out of 247 \n",
      "220 out of 247 \n",
      "240 out of 247 \n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "use_pickles = False\n",
    "if not use_pickles:\n",
    "    # BOW keywords fake\n",
    "    BOW_keywords_unreliable = BOWGenGlobal(BOWGen(df[df[\"type\"] != \"reliable\" ].meta_keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "779dc05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 out of 155 \n",
      "20 out of 155 \n",
      "40 out of 155 \n",
      "60 out of 155 \n",
      "80 out of 155 \n",
      "100 out of 155 \n",
      "120 out of 155 \n",
      "140 out of 155 \n",
      "Done\n",
      "0 out of 3 \n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "if not use_pickles:\n",
    "    # BOW whole text fake\n",
    "    BOW_Whole_text_fake = BOWGenGlobal(BOWGen(df[df[\"type\"] == \"fake\"].stemmed))\n",
    "    # BOW whole text credible\n",
    "    BOW_Whole_text_reliable = BOWGenGlobal(BOWGen(df[df[\"type\"] == \"reliable\"].stemmed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed2749f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word            count  frequency\n",
       "$               176    0.005277     1\n",
       "plan            43     0.001289     1\n",
       "planes          5      0.000150     1\n",
       "planet          12     0.000360     1\n",
       "planetary       1      0.000030     1\n",
       "                                   ..\n",
       "feel            93     0.002788     1\n",
       "feeling         19     0.000570     1\n",
       "feelingdenial   1      0.000030     1\n",
       "feelinghealing  2      0.000060     1\n",
       "™️              2      0.000060     1\n",
       "Length: 16676, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code for saving/reading dataframe to/from disk:\n",
    "import pickle\n",
    "\n",
    "if use_pickles:\n",
    "    BOW_Whole_text = pickle.load(open('BOW_Whole_text.pickle', 'rb'))\n",
    "    BOW_keywords_fake = pickle.load(open('BOW_keywords_fake.pickle', 'rb'))\n",
    "    BOW_Whole_text_fake = pickle.load(open('BOW_Whole_text_fake.pickle', 'rb'))\n",
    "    BOW_Whole_text_reliable = pickle.load(open('BOW_Whole_text_reliable.pickle', 'rb'))\n",
    "else:\n",
    "    pickle.dump(BOW_Whole_text, open('BOW_Whole_text.pickle', 'wb'))\n",
    "    #pickle.dump(BOW_keywords_fake, open('BOW_keywords_fake.pickle', 'wb'))\n",
    "    pickle.dump(BOW_Whole_text_fake, open('BOW_Whole_text_fake.pickle', 'wb'))\n",
    "    pickle.dump(BOW_Whole_text_reliable, open('BOW_Whole_text_reliable.pickle', 'wb'))\n",
    "\n",
    "BOW_Whole_text.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52b9058",
   "metadata": {},
   "source": [
    "## Task 3.1\n",
    "\n",
    "As our final result we will be calculating the frequncy delta between fake and reliable.\n",
    "And plotting it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a268c8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d611c166",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'frequency_x'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/DS/work/Milestone01_DS22/venv/lib/python3.10/site-packages/pandas/core/indexes/base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3620\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/Documents/DS/work/Milestone01_DS22/venv/lib/python3.10/site-packages/pandas/_libs/index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Documents/DS/work/Milestone01_DS22/venv/lib/python3.10/site-packages/pandas/_libs/index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'frequency_x'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m merged \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(\n\u001b[1;32m      6\u001b[0m     BOW_Whole_text_reliable,\n\u001b[1;32m      7\u001b[0m     BOW_Whole_text_fake,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     indicator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m merged \u001b[38;5;241m=\u001b[39m merged\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_x\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_y\u001b[39m\u001b[38;5;124m\"\u001b[39m],axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m merged[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_x\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mmerged\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_x\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     15\u001b[0m merged[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_y\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m merged[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     16\u001b[0m merged[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount_x\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m merged[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount_x\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/DS/work/Milestone01_DS22/venv/lib/python3.10/site-packages/pandas/core/frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3505\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3507\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/Documents/DS/work/Milestone01_DS22/venv/lib/python3.10/site-packages/pandas/core/indexes/base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3623\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3624\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3625\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3626\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3627\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'frequency_x'"
     ]
    }
   ],
   "source": [
    "BOW_Whole_text_fake[\"frequency\"] = BOW_Whole_text_fake[\"count\"]/BOW_Whole_text_fake.size\n",
    "BOW_Whole_text_reliable[\"frequency\"] = BOW_Whole_text_fake[\"count\"]/BOW_Whole_text_fake.size\n",
    "\n",
    "#BOW_Whole_text_reliable[\"Windex\"] = BOW_Whole_text_reliable[\"word\"]\n",
    "merged = pd.merge(\n",
    "    BOW_Whole_text_reliable,\n",
    "    BOW_Whole_text_fake,\n",
    "    how='left',\n",
    "    on=\"word\",\n",
    "    indicator=False\n",
    ")\n",
    "\n",
    "merged[\"frequency_x\"] = merged[\"frequency_x\"].fillna(0)\n",
    "merged[\"frequency_y\"] = merged[\"frequency_y\"].fillna(0)\n",
    "merged[\"count_x\"] = merged[\"count_x\"].fillna(0)\n",
    "merged[\"count_y\"] = merged[\"count_y\"].fillna(0)\n",
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854372c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "threeone = pd.concat(\n",
    "    [\n",
    "        merged[\"word\"],\n",
    "        merged[\"frequency_x\"] - merged[\"frequency_y\"], #fake is negative\n",
    "        merged[\"count_x\"] + merged[\"count_y\"]],\n",
    "    axis=1,\n",
    ")\n",
    "threeone.columns=[\"word\", \"freq_delta\", \"total_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b624b591",
   "metadata": {},
   "outputs": [],
   "source": [
    "threeone.sort_values(by=\"freq_delta\", ascending=True)[threeone.sort_values(by=\"freq_delta\", ascending=True)[\"total_count\"]>1].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d4f90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "threeone.sort_values(by=\"freq_delta\", ascending=False)[threeone.sort_values(by=\"freq_delta\", ascending=False)[\"total_count\"]>1].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac55d02",
   "metadata": {},
   "source": [
    "The fake news are correlated with common words sich as \"and\", \"the\" etc..\n",
    "While the reliable articles had the highest frequency delta with less used words such as investigator or restore,\n",
    "This is likley due to the reletivly small sample size for reliable news."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78662e7f",
   "metadata": {},
   "source": [
    "## 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a44108b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW_keywords_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a4c283",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e456c4ae",
   "metadata": {},
   "source": [
    "## 3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc678f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW_Whole_text.sort_values(by=\"count\", ascending=False)[\"count\"][:100].plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17b41ba",
   "metadata": {},
   "source": [
    "We can se that the top 100 words follows the zipf law. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15600a11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513e8d27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93e7dc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
