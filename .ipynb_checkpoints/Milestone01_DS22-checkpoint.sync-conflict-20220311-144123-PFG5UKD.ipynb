{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8bbb817",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "\n",
    "%\n",
    "If you haven't already done so, the first task is to form a group of 2-4 people. There is an announcement on Absalon describing how to do this. Make sure that you list the names of all members of the group at the top of the jupyter notebook along with your group number.\n",
    "%\n",
    "\n",
    "## Group members:\n",
    "* Oleksandr kryshtalov [mdq842]  \n",
    "* Ulrik Bj√∏rn Meelby [na]  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33adf0d",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "%\n",
    "For our fake news predictor, we will be using the FakeNewsCorpus dataset as our primary dataset. \n",
    "It is available from this github repository:, where you can also find information about how the data is collected, the available fields, etc. In this first milestone, we will work only on a small subset of the FakeNewsCorpus dataset. \n",
    "Your first task is to retrieve this subset from https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv and structure/process/clean it. Describe which procedures (and which libraries) you used and why they are appropriate.\n",
    "%\n",
    "\n",
    "In order to clean the data we need to first know what the primary goal of cleaning it is. \n",
    "In task 3 we have chosen to analyze following questions:\n",
    "* Are there words that have a higher frequency in fake-news articles over trustworthy ones, and what are those?\n",
    "* What are the meta_keywords that have the highest inclination to be fake?\n",
    "* Observe zipf law on english text with given dataset.\n",
    "\n",
    "All observations require text tokenization, and a way to clean and stem them, in order to not differentiate between words, eg. word and words.\n",
    "\n",
    "For the cleaning of the text we will be using cleatext.\n",
    "nltk will be used for tokinization via word_tokenize, and for stemming via the porter stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41c7666f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports:\n",
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "from cleantext import clean\n",
    "import nltk\n",
    "#from nltk.corpus import stopwords\n",
    "#from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e007ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We get the data from the web \n",
    "csvdata = requests.get(\"https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv\")\n",
    "csvdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ad09325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write csvfile\n",
    "with open(\"rawdata.csv\",\"wb\") as file:\n",
    "    file.write(csvdata.content)\n",
    "\n",
    "# Parse csv into dataframe\n",
    "dfRaw = pd.read_csv ('rawdata.csv')\n",
    "\n",
    "# drop some tables for brevity:\n",
    "df = dfRaw.iloc[: , 1:]\n",
    "df = df.drop(columns =[ \"url\", \"scraped_at\", \"scraped_at\", \"inserted_at\", \"updated_at\", \"keywords\", \"summary\"])\n",
    "\n",
    "# have a peek at the data:\n",
    "#df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d45a8c7",
   "metadata": {},
   "source": [
    "## Cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e12ca925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to make a pd.Series -> pd.Series function that cleans all the text\n",
    "# lowercases everything removes urls, emails etc.\n",
    "\n",
    "def clean_text(textSeries:pd.Series) -> pd.Series:\n",
    "    def clean_func (text):\n",
    "        return clean(text,\n",
    "            fix_unicode=True,            \n",
    "            to_ascii=True,               \n",
    "            lower=True,                  \n",
    "            no_line_breaks=True,        \n",
    "            no_urls=True,                \n",
    "            no_emails=True,              \n",
    "            no_phone_numbers=False,      \n",
    "            no_numbers=True,             \n",
    "            no_punct=True,              \n",
    "            replace_with_punct=\"\",       \n",
    "            replace_with_url=\"oURL\", # clean_text lowers after tagging for some reason???\n",
    "            replace_with_email=\"oEMAIL\",\n",
    "            replace_with_phone_number=\"oPHONE\",\n",
    "            replace_with_number=\"oNUM\",\n",
    "            replace_with_digit=\"0\",\n",
    "            lang=\"en\")\n",
    "    return textSeries.apply(clean_func)\n",
    "            \n",
    "df.content = clean_text(df.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad34b170",
   "metadata": {},
   "source": [
    "## Tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36def21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/olekkr/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt') # fix\n",
    "\n",
    "# Same as before a pd.Series -> pd.Series function.\n",
    "def tokenize(textSeries:pd.Series) -> pd.Series:\n",
    "    return pd.Series(textSeries.apply(nltk.tokenize.wordpunct_tokenize),name=\"tokens\")\n",
    "df[\"tokens\"] = tokenize(df.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6e2494",
   "metadata": {},
   "source": [
    "## Removal of stopwords:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4d826788",
   "metadata": {},
   "source": [
    "# a pd.Series -> pd.Series function to remove stopwords:\n",
    "def removeStopWords(tokens:pd.Series) -> pd.Series:\n",
    "    def removeStops(tokens):\n",
    "        sWords = nltk.corpus.stopwords.words('english')\n",
    "        return [token for token in tokens if not token in sWords] #duplicates are not purged at this stage.\n",
    "    return tokens.apply(removeStops)\n",
    "df[\"tokens_nostops\"] = removeStopWords(df.tokens)\n",
    "\n",
    "\n",
    "\n",
    "# might remove this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11efdce9",
   "metadata": {},
   "source": [
    "## Stemming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0ecb79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stems all the tokens\n",
    "def stem (data:pd.Series)-> pd.Series:\n",
    "    ps = PorterStemmer()\n",
    "    # function to be applied on each element in series\n",
    "    def stem (tokens):\n",
    "        return [ps.stem(token) for token in tokens]\n",
    "    return pd.Series(data.apply(stem))\n",
    "\n",
    "# removal of duplicates\n",
    "def dupeRemove (tokens:pd.Series) -> pd.Series:\n",
    "    def unDupe (tokens):\n",
    "        return list(set(tokens))\n",
    "    return tokens.apply(unDupe)\n",
    "\n",
    "\n",
    "\n",
    "df[\"stemmed_with_dupes\"] = stem(df.tokens)\n",
    "df[\"stemmed\"] = dupeRemove(df.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b7a03c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b88ecb0",
   "metadata": {},
   "source": [
    "# Task 3\n",
    "%\\\n",
    "Now try to explore the FakeNewsCorpus dataset. Make at least three non-trivial observations/discoveries about the data. These observations could be related to outliers, artefacts, or even better: genuinely interesting patterns in the data that could potentially be used for fake-news detection. Examples of simple observations could be how many missing values there are in particular columns - or what the distribution over domains is. Be creative! :).  \n",
    "%\n",
    "\n",
    "We found some observations we wanted to look into in Task 2: \n",
    "1. Are there words that have a higher frequency in fake-news articles over trustworthy ones, and what are those?\n",
    "2. What are the meta_keywords that have the highest inclination to be fake?\n",
    "3. Observe zipf law on english text with given dataset.\n",
    "\n",
    "These tasks can then be split into smaller tasks:\n",
    "### 1\n",
    "Generate BOW for fake and credibile articles seperatly. \\\n",
    "For each word find credible/fake ratio of the frequencies. \n",
    "\n",
    "\n",
    "### 2\n",
    "Find all keywords \\\n",
    "Iterate over each fake/credible adding to each each frequency counter. \\\n",
    "Visualize \n",
    "\n",
    "\n",
    "## 3\n",
    "Create a vocabulary for the entire dataset \\\n",
    "calculate bag of words for all text. \\\n",
    "Sort BOW by frequency. \\\n",
    "plot on a scatterplot. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a430b7",
   "metadata": {},
   "source": [
    "## creating a vocabulary for enitre corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76f69720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% this bad.\n",
    "## creating a vocabulary for enitre corpus\n",
    "def genVocab(tokens:pd.Series)-> pd.Series:\n",
    "    vocab = []\n",
    "    for (index, value) in tokens.items():\n",
    "        vocab = value + vocab\n",
    "    return pd.Series(list(set(vocab)), name=\"vocab\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3362e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# kinda bad O(n^2) scaling but good enough for this dataset.\n",
    "vocab = genVocab(df.stemmed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e19214c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 out of 250 \n",
      "40 out of 250 \n",
      "60 out of 250 \n",
      "80 out of 250 \n",
      "100 out of 250 \n",
      "120 out of 250 \n",
      "140 out of 250 \n",
      "160 out of 250 \n",
      "180 out of 250 \n",
      "200 out of 250 \n",
      "220 out of 250 \n",
      "240 out of 250 \n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "#%%timeit\n",
    "# In the following section we have BOW code, to generate bags of words\n",
    "# and combining them. As previous we well be using pd.Series as out datatype.\n",
    "# The code takes a Long time to complete, might want to skip this section and \n",
    "# instead unpickle. \n",
    "\n",
    "\n",
    "# generate BOW given a set of tokens(with dupes).\n",
    "def BOWGen(tokenss:pd.Series) -> pd.DataFrame: # with 2 cols for word and count\n",
    "    \n",
    "    #generate Bow for 1 list of tokens\n",
    "    def _BOWGen(tokens:list[str]) -> (list,list): # with 2 cols for word and count\n",
    "        vocab = []\n",
    "        freqs = []\n",
    "        for token in tokens:\n",
    "            if token not in vocab:\n",
    "                vocab.append(token)\n",
    "                freqs.append(1)\n",
    "            else:\n",
    "                # increment frequncy counter by one.\n",
    "                freqs[vocab.index(token)] += 1\n",
    "        return vocab,freqs\n",
    "    return tokenss.apply(_BOWGen)\n",
    "\n",
    "\n",
    "def BOWGenGlobal(BOWs:pd.Series) -> pd.DataFrame:\n",
    "    def mergeBOWs(a:(list[str],list[int]), b):\n",
    "        vocab, freqs = a\n",
    "        for (word, count) in zip(b[0],b[1]):\n",
    "            if word not in vocab:\n",
    "                vocab.append(word)\n",
    "                freqs.append(count)\n",
    "            else:\n",
    "                freqs[vocab.index(word)] += count\n",
    "        return vocab, freqs\n",
    "    \n",
    "    BOWList = [i for (_, i) in BOWs.items()]\n",
    "    \n",
    "    # accumulator\n",
    "    _BOW = BOWList[0]  \n",
    "    for idx, BOW in enumerate(BOWList[1:]):\n",
    "        _BOW = mergeBOWs(BOW, _BOW)\n",
    "        if (idx+1) % 20 == 0:\n",
    "            print(f\"{idx+1} out of {len(BOWList)} \")\n",
    "    print(\"Done\")\n",
    "    return pd.DataFrame(np.transpose(_BOW), columns=[\"word\", \"count\"])\n",
    "      \n",
    "# commented for sanity\n",
    "#BOWs = BOWGenGlobal(BOWGen(df.tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2770861d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to pickle BOW\n",
    "import pickle\n",
    "\n",
    "# Uncomment to use stored version\n",
    "\n",
    "#with open(\"FullTextBOW.pickle\", \"wb\") as file:\n",
    "#    pickle.dump(BOWs, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "75a1ee7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to unpickle \n",
    "with open(\"FullTextBOW.pickle\", \"rb\") as file:\n",
    "    BOWs = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e228e50a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>former</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>us</td>\n",
       "      <td>447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>president</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bill</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>clinton</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16671</th>\n",
       "      <td>potency</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16672</th>\n",
       "      <td>pitched</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16673</th>\n",
       "      <td>waffle</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16674</th>\n",
       "      <td>split</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16675</th>\n",
       "      <td>empathy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16676 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            word count\n",
       "0         former    57\n",
       "1             us   447\n",
       "2      president   175\n",
       "3           bill    47\n",
       "4        clinton    75\n",
       "...          ...   ...\n",
       "16671    potency     1\n",
       "16672    pitched     1\n",
       "16673     waffle     4\n",
       "16674      split     2\n",
       "16675    empathy     1\n",
       "\n",
       "[16676 rows x 2 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d92a76f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
