{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8bbb817",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "\n",
    "%\n",
    "If you haven't already done so, the first task is to form a group of 2-4 people. There is an announcement on Absalon describing how to do this. Make sure that you list the names of all members of the group at the top of the jupyter notebook along with your group number.\n",
    "%\n",
    "\n",
    "## Group members:\n",
    "* Oleksandr kryshtalov [mdq842]  \n",
    "* Ulrik Bj√∏rn Meelby [na]  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33adf0d",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "%\n",
    "For our fake news predictor, we will be using the FakeNewsCorpus dataset as our primary dataset. \n",
    "It is available from this github repository:, where you can also find information about how the data is collected, the available fields, etc. In this first milestone, we will work only on a small subset of the FakeNewsCorpus dataset. \n",
    "Your first task is to retrieve this subset from https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv and structure/process/clean it. Describe which procedures (and which libraries) you used and why they are appropriate.\n",
    "%\n",
    "\n",
    "In order to clean the data we need to first know what the primary goal of cleaning it is. \n",
    "In task 3 we have chosen to analyze following questions:\n",
    "* Are there words that have a higher frequency in fake-news articles over trustworthy ones, and what are those?\n",
    "* What are the meta_keywords that have the highest inclination to be fake?\n",
    "* Observe zipf law on english text with given dataset.\n",
    "\n",
    "All observations require text tokenization, and a way to clean and stem them, in order to not differentiate between words, eg. word and words.\n",
    "\n",
    "For the cleaning of the text we will be using cleatext.\n",
    "nltk will be used for tokinization via word_tokenize, and for stemming via the porter stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41c7666f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports:\n",
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "from cleantext import clean\n",
    "import nltk\n",
    "#from nltk.corpus import stopwords\n",
    "#from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e007ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We get the data from the web \n",
    "csvdata = requests.get(\"https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv\")\n",
    "csvdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ad09325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write csvfile\n",
    "with open(\"rawdata.csv\",\"wb\") as file:\n",
    "    file.write(csvdata.content)\n",
    "\n",
    "# Parse csv into dataframe\n",
    "dfRaw = pd.read_csv ('rawdata.csv')\n",
    "\n",
    "# drop some tables for brevity:\n",
    "df = dfRaw.iloc[: , 1:]\n",
    "df = df.drop(columns =[ \"url\", \"scraped_at\", \"scraped_at\", \"inserted_at\", \"updated_at\", \"keywords\", \"summary\"])\n",
    "\n",
    "# have a peek at the data:\n",
    "#df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d45a8c7",
   "metadata": {},
   "source": [
    "## Cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e12ca925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to make a pd.Series -> pd.Series function that cleans all the text\n",
    "# lowercases everything removes urls, emails etc.\n",
    "\n",
    "def clean_text(textSeries:pd.Series) -> pd.Series:\n",
    "    def clean_func (text):\n",
    "        return clean(text,\n",
    "            fix_unicode=True,            \n",
    "            to_ascii=True,               \n",
    "            lower=True,                  \n",
    "            no_line_breaks=True,        \n",
    "            no_urls=True,                \n",
    "            no_emails=True,              \n",
    "            no_phone_numbers=False,      \n",
    "            no_numbers=True,             \n",
    "            no_punct=True,              \n",
    "            replace_with_punct=\"\",       \n",
    "            replace_with_url=\"oURL\", # clean_text lowers after tagging for some reason???\n",
    "            replace_with_email=\"oEMAIL\",\n",
    "            replace_with_phone_number=\"oPHONE\",\n",
    "            replace_with_number=\"oNUM\",\n",
    "            replace_with_digit=\"0\",\n",
    "            lang=\"en\")\n",
    "    return textSeries.apply(clean_func)\n",
    "            \n",
    "df.content = clean_text(df.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad34b170",
   "metadata": {},
   "source": [
    "## Tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36def21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Oleks\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt') # fix\n",
    "\n",
    "# Same as before a pd.Series -> pd.Series function.\n",
    "def tokenize(textSeries:pd.Series) -> pd.Series:\n",
    "    return pd.Series(textSeries.apply(nltk.tokenize.wordpunct_tokenize),name=\"tokens\")\n",
    "df[\"tokens\"] = tokenize(df.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6e2494",
   "metadata": {},
   "source": [
    "## Removal of stopwords:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4d826788",
   "metadata": {},
   "source": [
    "# a pd.Series -> pd.Series function to remove stopwords:\n",
    "def removeStopWords(tokens:pd.Series) -> pd.Series:\n",
    "    def removeStops(tokens):\n",
    "        sWords = nltk.corpus.stopwords.words('english')\n",
    "        return [token for token in tokens if not token in sWords] #duplicates are not purged at this stage.\n",
    "    return tokens.apply(removeStops)\n",
    "df[\"tokens_nostops\"] = removeStopWords(df.tokens)\n",
    "\n",
    "\n",
    "\n",
    "# might remove this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11efdce9",
   "metadata": {},
   "source": [
    "## Stemming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0ecb79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stems all the tokens\n",
    "def stem (data:pd.Series)-> pd.Series:\n",
    "    ps = PorterStemmer()\n",
    "    # function to be applied on each element in series\n",
    "    def stem (tokens):\n",
    "        return [ps.stem(token) for token in tokens]\n",
    "    return pd.Series(data.apply(stem))\n",
    "\n",
    "# removal of duplicates\n",
    "def dupeRemove (tokens:pd.Series) -> pd.Series:\n",
    "    def unDupe (tokens):\n",
    "        return list(set(tokens))\n",
    "    return tokens.apply(unDupe)\n",
    "\n",
    "\n",
    "\n",
    "df[\"stemmed_with_dupes\"] = stem(df.tokens)\n",
    "df[\"stemmed\"] = dupeRemove(df.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b7a03c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b88ecb0",
   "metadata": {},
   "source": [
    "# Task 3\n",
    "%\\\n",
    "Now try to explore the FakeNewsCorpus dataset. Make at least three non-trivial observations/discoveries about the data. These observations could be related to outliers, artefacts, or even better: genuinely interesting patterns in the data that could potentially be used for fake-news detection. Examples of simple observations could be how many missing values there are in particular columns - or what the distribution over domains is. Be creative! :).  \n",
    "%\n",
    "\n",
    "We found some observations we wanted to look into in Task 2: \n",
    "1. Are there words that have a higher frequency in fake-news articles over trustworthy ones, and what are those?\n",
    "2. What are the meta_keywords that have the highest inclination to be fake?\n",
    "3. Observe zipf law on english text with given dataset.\n",
    "\n",
    "These tasks can then be split into smaller tasks:\n",
    "### 1\n",
    "\n",
    "\n",
    "### 2\n",
    "Find all keywords \\\n",
    "Iterate over each fake/credible adding to each each frequency counter.\\\n",
    "\n",
    "\n",
    "## 3\n",
    "Create a vocabulary for the entire dataset \\\n",
    "calculate bag of words for all text. \\\n",
    "Sort BOW by frequency. \\\n",
    "plot on a scatterplot. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f69720",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3362e5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
