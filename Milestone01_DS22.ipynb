{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8bbb817",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "\n",
    "%\n",
    "If you haven't already done so, the first task is to form a group of 2-4 people. There is an announcement on Absalon describing how to do this. Make sure that you list the names of all members of the group at the top of the jupyter notebook along with your group number.\n",
    "%\n",
    "\n",
    "## Group members:\n",
    "* Oleksandr kryshtalov [mdq842]  \n",
    "* Ulrik Bj√∏rn Meelby [na]  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33adf0d",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "%\n",
    "For our fake news predictor, we will be using the FakeNewsCorpus dataset as our primary dataset. \n",
    "It is available from this github repository:, where you can also find information about how the data is collected, the available fields, etc. In this first milestone, we will work only on a small subset of the FakeNewsCorpus dataset. \n",
    "Your first task is to retrieve this subset from https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv and structure/process/clean it. Describe which procedures (and which libraries) you used and why they are appropriate.\n",
    "%\n",
    "\n",
    "In order to clean the data we need to first know what the primary goal of cleaning it is. \n",
    "In task 3 we have chosen to analyze following questions:\n",
    "* Are there words that have a higher frequency in fake-news articles over trustworthy ones, and what are those?\n",
    "* What are the meta_keywords that have the highest inclination to be fake?\n",
    "* Observe zipf law on english text with given dataset.\n",
    "\n",
    "All observations require text tokenization, and a way to clean and stem them, in order to not differentiate between words, eg. word and words.\n",
    "\n",
    "For the cleaning of the text we will be using cleatext.\n",
    "nltk will be used for tokinization via word_tokenize, and for stemming via the porter stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41c7666f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports:\n",
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "from cleantext import clean\n",
    "import nltk\n",
    "#from nltk.corpus import stopwords\n",
    "#from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e007ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We get the data from the web \n",
    "csvdata = requests.get(\"https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv\")\n",
    "csvdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ad09325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write csvfile\n",
    "with open(\"rawdata.csv\",\"wb\") as file:\n",
    "    file.write(csvdata.content)\n",
    "\n",
    "# Parse csv into dataframe\n",
    "dfRaw = pd.read_csv ('rawdata.csv')\n",
    "\n",
    "# drop some tables for brevity:\n",
    "df = dfRaw.iloc[: , 1:]\n",
    "df = df.drop(columns =[ \"url\", \"scraped_at\", \"scraped_at\", \"inserted_at\", \"updated_at\", \"keywords\", \"summary\"])\n",
    "\n",
    "# have a peek at the data:\n",
    "#df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d45a8c7",
   "metadata": {},
   "source": [
    "## Cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e12ca925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to make a pd.Series -> pd.Series function that cleans all the text\n",
    "# lowercases everything removes urls, emails etc.\n",
    "\n",
    "def clean_text(textSeries:pd.Series) -> pd.Series:\n",
    "    def clean_func (text):\n",
    "        return clean(text,\n",
    "            fix_unicode=True,            \n",
    "            to_ascii=True,               \n",
    "            lower=True,                  \n",
    "            no_line_breaks=True,        \n",
    "            no_urls=True,                \n",
    "            no_emails=True,              \n",
    "            no_phone_numbers=False,      \n",
    "            no_numbers=True,             \n",
    "            no_punct=True,              \n",
    "            replace_with_punct=\"\",       \n",
    "            replace_with_url=\"oURL\", # clean_text lowers after tagging for some reason???\n",
    "            replace_with_email=\"oEMAIL\",\n",
    "            replace_with_phone_number=\"oPHONE\",\n",
    "            replace_with_number=\"oNUM\",\n",
    "            replace_with_digit=\"0\",\n",
    "            lang=\"en\")\n",
    "    return textSeries.apply(clean_func)\n",
    "            \n",
    "df.content = clean_text(df.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad34b170",
   "metadata": {},
   "source": [
    "## Tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36def21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/olekkr/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt') # fix\n",
    "\n",
    "# Same as before a pd.Series -> pd.Series function.\n",
    "def tokenize(textSeries:pd.Series) -> pd.Series:\n",
    "    return pd.Series(textSeries.apply(nltk.tokenize.wordpunct_tokenize),name=\"tokens\")\n",
    "df[\"tokens\"] = tokenize(df.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6e2494",
   "metadata": {},
   "source": [
    "## Removal of stopwords:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4d826788",
   "metadata": {},
   "source": [
    "# a pd.Series -> pd.Series function to remove stopwords:\n",
    "def removeStopWords(tokens:pd.Series) -> pd.Series:\n",
    "    def removeStops(tokens):\n",
    "        sWords = nltk.corpus.stopwords.words('english')\n",
    "        return [token for token in tokens if not token in sWords] #duplicates are not purged at this stage.\n",
    "    return tokens.apply(removeStops)\n",
    "df[\"tokens_nostops\"] = removeStopWords(df.tokens)\n",
    "\n",
    "\n",
    "\n",
    "# might remove this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11efdce9",
   "metadata": {},
   "source": [
    "## Stemming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0ecb79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stems all the tokens\n",
    "def stem (data:pd.Series)-> pd.Series:\n",
    "    ps = PorterStemmer()\n",
    "    # function to be applied on each element in series\n",
    "    def stem (tokens):\n",
    "        return [ps.stem(token) for token in tokens]\n",
    "    return pd.Series(data.apply(stem))\n",
    "\n",
    "# removal of duplicates\n",
    "def dupeRemove (tokens:pd.Series) -> pd.Series:\n",
    "    def unDupe (tokens):\n",
    "        return list(set(tokens))\n",
    "    return tokens.apply(unDupe)\n",
    "\n",
    "\n",
    "\n",
    "df[\"stemmed_with_dupes\"] = stem(df.tokens)\n",
    "df[\"stemmed\"] = dupeRemove(df.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b88ecb0",
   "metadata": {},
   "source": [
    "# Task 3\n",
    "%\\\n",
    "Now try to explore the FakeNewsCorpus dataset. Make at least three non-trivial observations/discoveries about the data. These observations could be related to outliers, artefacts, or even better: genuinely interesting patterns in the data that could potentially be used for fake-news detection. Examples of simple observations could be how many missing values there are in particular columns - or what the distribution over domains is. Be creative! :).  \n",
    "%\n",
    "\n",
    "We found some observations we wanted to look into in Task 2: \n",
    "1. Are there words that have a higher frequency in fake-news articles over trustworthy ones, and what are those?\n",
    "2. What are the meta_keywords that have the highest inclination to be fake?\n",
    "3. Observe zipf law on english text with given dataset.\n",
    "\n",
    "These tasks can then be split into smaller tasks:\n",
    "### 1\n",
    "* Generate BOW for fake and credibile articles seperatly. \n",
    "* For each word find credible/fake ratio of the frequencies. \n",
    "\n",
    "\n",
    "### 2\n",
    "* Create BOW for meta keywords for fakes\n",
    "* Sort \n",
    "* Visualize this coefficant.\n",
    "\n",
    "\n",
    "## 3\n",
    "* Create a vocabulary for the entire dataset \n",
    "* calculate bag of words for all text. \n",
    "* Sort BOW by frequency. \n",
    "* plot on a scatterplot. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a430b7",
   "metadata": {},
   "source": [
    "## Creating BOWs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "323bf11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 out of 10 \n",
      "Done\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>the</td>\n",
       "      <td>299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>onum</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>of</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>to</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>in</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     word  count\n",
       "19    the    299\n",
       "94   onum    175\n",
       "39     of    155\n",
       "1      to    112\n",
       "109    in    105"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In the following section we have BOW code, to generate bags of words\n",
    "# and combining them. As previous we well be using pd.Series as out datatype.\n",
    "# The code takes a Long time to complete, might want to skip this section and \n",
    "# instead unpickle. \n",
    "\n",
    "\n",
    "# generate BOW given a set of tokens(with dupes).\n",
    "def BOWGen(tokenss:pd.Series) -> pd.DataFrame: # with 2 cols for word and count\n",
    "    \n",
    "    #generate Bow for 1 list of tokens\n",
    "    def _BOWGen(tokens:list[str]) -> (list,list): # with 2 cols for word and count\n",
    "        vocab = []\n",
    "        freqs = []\n",
    "        for token in tokens:\n",
    "            if token not in vocab:\n",
    "                vocab.append(token)\n",
    "                freqs.append(1)\n",
    "            else:\n",
    "                # increment frequncy counter by one.\n",
    "                freqs[vocab.index(token)] += 1\n",
    "        return vocab,freqs\n",
    "    return tokenss.apply(_BOWGen)\n",
    "\n",
    "\n",
    "def BOWGenGlobal(BOWs:pd.Series) -> pd.DataFrame:\n",
    "    def mergeBOWs(a:(list[str],list[int]), b):\n",
    "        vocab, freqs = a\n",
    "        for (word, count) in zip(b[0],b[1]):\n",
    "            if word not in vocab:\n",
    "                vocab.append(word)\n",
    "                freqs.append(count)\n",
    "            else:\n",
    "                freqs[vocab.index(word)] += count\n",
    "        return vocab, freqs\n",
    "    \n",
    "    BOWList = [i for (_, i) in BOWs.items()]\n",
    "    \n",
    "    # accumulator\n",
    "    _BOW = ([],[])\n",
    "    for idx, BOW in enumerate(BOWList):\n",
    "        _BOW = mergeBOWs(BOW, _BOW)\n",
    "        if (idx) % 20 == 0:\n",
    "            print(f\"{idx} out of {len(BOWList)} \")\n",
    "    out = pd.DataFrame((_BOW)).transpose()\n",
    "    out.columns = [\"word\", \"count\"]\n",
    "    out[\"count\"] = out[\"count\"].astype(int)\n",
    "    print(\"Done\")\n",
    "    return out\n",
    "      \n",
    "# test \n",
    "BOWs = BOWGenGlobal(BOWGen(df.tokens.iloc[0:10]))\n",
    "BOWs.sort_values(\"count\",ascending=False).iloc[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7d8025",
   "metadata": {},
   "source": [
    "## Generating all the BOWs we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45ca2b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 out of 250 \n",
      "20 out of 250 \n",
      "40 out of 250 \n",
      "60 out of 250 \n",
      "80 out of 250 \n",
      "100 out of 250 \n",
      "120 out of 250 \n",
      "140 out of 250 \n",
      "160 out of 250 \n",
      "180 out of 250 \n",
      "200 out of 250 \n",
      "220 out of 250 \n",
      "240 out of 250 \n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# BOW Whole text\n",
    "#BOW_Whole_text = BOWGenGlobal(BOWGen(df.tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4683038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"FullTextBOW.pickle\", \"wb\") as file:\n",
    "    #pickle.dump(BOW_Whole_text, file)\n",
    "    pass\n",
    "with open(\"FullTextBOW.pickle\", \"rb\") as file:\n",
    "    #BOW_Whole_text = pickle.load(file)\n",
    "    pass\n",
    "# dont lose your pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02bb0cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 out of 155 \n",
      "20 out of 155 \n",
      "40 out of 155 \n",
      "60 out of 155 \n",
      "80 out of 155 \n",
      "100 out of 155 \n",
      "120 out of 155 \n",
      "140 out of 155 \n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# BOW keywords fake\n",
    "BOW_keywords_fake = BOWGenGlobal(BOWGen(df[df[\"type\"] == \"fake\"].meta_keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "779dc05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 out of 155 \n",
      "20 out of 155 \n",
      "40 out of 155 \n",
      "60 out of 155 \n",
      "80 out of 155 \n",
      "100 out of 155 \n",
      "120 out of 155 \n",
      "140 out of 155 \n",
      "Done\n",
      "0 out of 3 \n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# BOW whole text fake\n",
    "BOW_Whole_text_fake = BOWGenGlobal(BOWGen(df[df[\"type\"] == \"fake\"].stemmed))\n",
    "# BOW whole text credible\n",
    "BOW_Whole_text_fake = BOWGenGlobal(BOWGen(df[df[\"type\"] == \"reliable\"].stemmed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf3b03c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
